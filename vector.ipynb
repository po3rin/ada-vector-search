{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import pandas as pd\n",
    "import base64\n",
    "from itertools import zip_longest, filterfalse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_INDEX=True\n",
    "IS_TEST=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(dotenv_path='.local.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings: tensor([[ 0.1376,  0.5348,  0.0560,  ...,  0.9439,  0.1351,  0.0652],\n",
      "        [ 0.0604,  0.6181, -0.5663,  ...,  0.0606, -0.8503,  0.1538]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SentenceBertJapanese:\n",
    "    \"\"\"\n",
    "    https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name_or_path, device=None):\n",
    "        self.tokenizer = BertJapaneseTokenizer.from_pretrained(model_name_or_path)\n",
    "        self.model = BertModel.from_pretrained(model_name_or_path)\n",
    "        self.model.eval()\n",
    "\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "    def encode(self, sentences, batch_size=8):\n",
    "        all_embeddings = []\n",
    "        iterator = range(0, len(sentences), batch_size)\n",
    "        for batch_idx in iterator:\n",
    "            batch = sentences[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            encoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"longest\", \n",
    "                                           truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\n",
    "\n",
    "            all_embeddings.extend(sentence_embeddings)\n",
    "\n",
    "        # return torch.stack(all_embeddings).numpy()\n",
    "        return torch.stack(all_embeddings)\n",
    "\n",
    "\n",
    "MODEL_NAME = \"sonoisa/sentence-bert-base-ja-mean-tokens-v2\"  # <- v2です。\n",
    "model = SentenceBertJapanese(MODEL_NAME)\n",
    "\n",
    "sentences = [\"暴走したAI\", \"暴走した人工知能\"]\n",
    "sentence_embeddings = model.encode(sentences, batch_size=8)\n",
    "\n",
    "print(\"Sentence embeddings:\", sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 何かしら素敵なデータをCSVで用意しておく\n",
    "df = pd.read_csv('resources/data.csv', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20550\n"
     ]
    }
   ],
   "source": [
    "titles = df['title'].to_list()\n",
    "print(len(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://askd-qa-v2.es.ap-northeast-1.aws.found.io:9243\n",
      "elastic\n",
      "cfpewA9RsLfvWcRADa2HEEg7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TextApiResponse('green  open  .internal.alerts-observability.logs.alerts-default-000001    9beEw5cmQR6KwjuPRw26Lw 1 0     0 0   248b   248b\\ngreen  open  .internal.alerts-observability.uptime.alerts-default-000001  EwjOj6Y1QEyvCZoBycyVYA 1 0     0 0   248b   248b\\ngreen  open  .fleet-file-data-agent-000001                                PkCp0NzMTcCqxJtEVB29dA 1 0     0 0   248b   248b\\nyellow open  query_logs-v0.3                                              vL4u4C1XQQexTB1u95_cUg 2 2     0 0   496b   496b\\nyellow open  index                                                        8dG5KMxiRLybegWpfvCiWw 2 2     3 0 58.8kb 58.8kb\\ngreen  open  .fleet-files-agent-000001                                    Rx9P5HZKRjqHW_EvIbINdQ 1 0     0 0   248b   248b\\ngreen  open  .internal.alerts-observability.slo.alerts-default-000001     P_Gr-3uWQamSzAKlTqH8fQ 1 0     0 0   248b   248b\\nyellow open  topics-v0.28                                                 0aUKYqzbRmCVflvJMiJkDQ 2 2 50117 0   84mb   84mb\\nyellow open  articles-v0.5                                                GzLinysLSpyp68jy8jKFWA 2 2     0 0   496b   496b\\nyellow close topics-v0.27                                                 -PajtLbpQTaY2V6m2MOX2w 2 2                      \\ngreen  open  .internal.alerts-observability.apm.alerts-default-000001     gsNNvyOYRm22y0qlccc7yg 1 0     0 0   248b   248b\\nyellow open  special-v0.2                                                 sQfS32e4Rq-ZD8UiGzw65A 2 2     0 0   496b   496b\\nyellow open  special-v0.1                                                 J5pHFJCcTF2wqKosXS7uBA 2 2     0 0   496b   496b\\ngreen  open  .internal.alerts-observability.metrics.alerts-default-000001 jBAjImQ-RFaFNo4E57S9Dg 1 0     0 0   248b   248b\\ngreen  open  .internal.alerts-security.alerts-default-000001              qppwTqWrSGawW1K5jeh8Hw 1 0     0 0   248b   248b\\n')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ES_URL = os.environ.get(\"ES_URL\")\n",
    "ES_USER = os.environ.get(\"ES_USER\")\n",
    "ES_PASS = os.environ.get(\"ES_PASS\")\n",
    "INDEX_NAME = os.environ.get(\"ES_INDEX_NAME\")\n",
    "print(ES_URL)\n",
    "print(ES_USER)\n",
    "print(ES_PASS)\n",
    "\n",
    "es = Elasticsearch(ES_URL, basic_auth=(ES_USER, ES_PASS))\n",
    "es.cat.indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動作確認用\n",
    "if IS_TEST:\n",
    "    titles = titles[:5]\n",
    "    # titles = [\"新型コロナワクチン予防接種について\", \"胃ガンの手術\", '三歳の子供が熱']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(ln: list, n: int):\n",
    "    return zip_longest(*[iter(ln)]*n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_offset = 0\n",
    "for chunk in chunk(titles, n=8):\n",
    "    chunk = filter(None, chunk)\n",
    "    title_list = list(chunk)\n",
    "    sentence_embeddings = model.encode(title_list, batch_size=8)\n",
    "    actions = [{\n",
    "        \"_id\": index_offset + _id,\n",
    "        \"_source\": {\n",
    "            \"title\": title,\n",
    "            \"vector\": embedding,\n",
    "        },\n",
    "    } for _id, (title, embedding) in enumerate(zip(title_list, sentence_embeddings.tolist()))]\n",
    "    success, errors = bulk(es, actions, index=INDEX_NAME, refresh=True)\n",
    "    if len(errors) != 0:\n",
    "        raise Exception(errors[0])\n",
    "    index_offset += len(title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_title = '三歳児の高熱'\n",
    "search_embeddings = model.encode([search_title], batch_size=8)\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"term\": {\n",
    "            \"title\": search_title\n",
    "        }\n",
    "    },\n",
    "    \"knn\": {\n",
    "        \"field\": \"vector\",\n",
    "        \"query_vector\": search_embeddings.tolist()[0],\n",
    "        \"k\": 10,\n",
    "        \"num_candidates\": 50\n",
    "    },\n",
    "    \"rank\": {\n",
    "        \"rrf\": {\n",
    "            \"window_size\": 50,\n",
    "            \"rank_constant\": 20\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kl/fr_z14214psgvtywhz6p9d5m0000gs/T/ipykernel_6525/425148530.py:1: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  result = es.search(index=INDEX_NAME, body=query, size=10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = es.search(index=INDEX_NAME, body=query, size=10)\n",
    "# 検索結果からドキュメントの内容のみ表示\n",
    "for document in result[\"hits\"][\"hits\"]:\n",
    "    print(document[\"_source\"]['title'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8f399a0e8a3a80a2c82d1d3172123b67620ed0d50c52f8045d2574ddf666f43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
