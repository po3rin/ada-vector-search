{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_INDEX=True\n",
    "IS_TEST=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path='.local.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceBertJapanese:\n",
    "    \"\"\"\n",
    "    https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name_or_path, device=None):\n",
    "        self.tokenizer = BertJapaneseTokenizer.from_pretrained(model_name_or_path)\n",
    "        self.model = BertModel.from_pretrained(model_name_or_path)\n",
    "        self.model.eval()\n",
    "\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "    def encode(self, sentences, batch_size=8):\n",
    "        all_embeddings = []\n",
    "        iterator = range(0, len(sentences), batch_size)\n",
    "        for batch_idx in iterator:\n",
    "            batch = sentences[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            encoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"longest\", \n",
    "                                           truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\n",
    "\n",
    "            all_embeddings.extend(sentence_embeddings)\n",
    "\n",
    "        # return torch.stack(all_embeddings).numpy()\n",
    "        return torch.stack(all_embeddings)\n",
    "\n",
    "\n",
    "MODEL_NAME = \"sonoisa/sentence-bert-base-ja-mean-tokens-v2\"  # <- v2です。\n",
    "model = SentenceBertJapanese(MODEL_NAME)\n",
    "\n",
    "sentences = [\"暴走したAI\", \"暴走した人工知能\"]\n",
    "sentence_embeddings = model.encode(sentences, batch_size=8)\n",
    "\n",
    "print(\"Sentence embeddings:\", sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 何かしら素敵なデータをCSVで用意しておく\n",
    "df = pd.read_csv('resources/data.csv', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df['title'].to_list()\n",
    "print(len(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動作確認用\n",
    "if IS_TEST:\n",
    "    titles = [\"新型コロナワクチン予防接種について\", \"胃ガンの手術\", '三歳の子供が熱']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = model.encode(titles, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_URL = os.environ.get(\"ES_URL\")\n",
    "ES_USER = os.environ.get(\"ES_USER\")\n",
    "ES_PASS = os.environ.get(\"ES_PASS\")\n",
    "INDEX_NAME = os.environ.get(\"ES_INDEX_NAME\")\n",
    "print(ES_URL)\n",
    "print(ES_USER)\n",
    "print(ES_PASS)\n",
    "\n",
    "es = Elasticsearch(ES_URL, basic_auth=(ES_USER, ES_PASS))\n",
    "es.cat.indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [{\n",
    "    \"_id\": _id,\n",
    "    \"_source\": {\n",
    "        \"title\": title,\n",
    "        \"vector\": embedding,\n",
    "    },\n",
    "} for _id, (title, embedding) in enumerate(zip(titles, sentence_embeddings.tolist()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_INDEX:\n",
    "    success, errors = bulk(es, actions, index=INDEX_NAME, refresh=True)\n",
    "\n",
    "print(success)\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_title = '三歳児の高熱'\n",
    "search_embeddings = model.encode([search_title], batch_size=8)\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"term\": {\n",
    "            \"title\": search_title\n",
    "        }\n",
    "    },\n",
    "    \"knn\": {\n",
    "        \"field\": \"vector\",\n",
    "        \"query_vector\": search_embeddings.tolist()[0],\n",
    "        \"k\": 10,\n",
    "        \"num_candidates\": 50\n",
    "    },\n",
    "    \"rank\": {\n",
    "        \"rrf\": {\n",
    "            \"window_size\": 50,\n",
    "            \"rank_constant\": 20\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = es.search(index=INDEX_NAME, body=query, size=10)\n",
    "# 検索結果からドキュメントの内容のみ表示\n",
    "for document in result[\"hits\"][\"hits\"]:\n",
    "    print(document[\"_source\"]['title'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8f399a0e8a3a80a2c82d1d3172123b67620ed0d50c52f8045d2574ddf666f43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
